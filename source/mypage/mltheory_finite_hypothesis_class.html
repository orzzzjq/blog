<!DOCTYPE html><html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>SVM and Polytope Distance</title>
<!--Generated on Wed Jan 10 19:26:33 2024 by LaTeXML (version 0.8.6) http://dlmf.nist.gov/LaTeXML/.-->
<!--Document created on October 2023.-->

<link rel="stylesheet" href="css/ar5iv.min.css" type="text/css">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document">
<div id="S1.p2" class="ltx_para ltx_noindent">
<p class="ltx_p">As the very first example, we consider the binary classification problem, where the data points have features <math id="S1.p2.m1" class="ltx_Math" alttext="{\bm{x}}_{i}" display="inline"><msub><mi>ğ’™</mi><mi>i</mi></msub></math> from a domain set <math id="S1.p2.m2" class="ltx_Math" alttext="\mathcal{X}" display="inline"><mi class="ltx_font_mathcaligraphic">ğ’³</mi></math> and (deterministic) labels <math id="S1.p2.m3" class="ltx_Math" alttext="y_{i}" display="inline"><msub><mi>y</mi><mi>i</mi></msub></math> in <math id="S1.p2.m4" class="ltx_Math" alttext="\{0,1\}" display="inline"><mrow><mo stretchy="false">{</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="false">}</mo></mrow></math>. We assume that:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S1.I1.i1.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">The i.i.d.Â assumption: The examples in the training set <math id="S1.I1.i1.p1.m1" class="ltx_Math" alttext="\{{\bm{x}}_{1},\dots,{\bm{x}}_{m}\}" display="inline"><mrow><mo stretchy="false">{</mo><msub><mi>ğ’™</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant="normal">â€¦</mi><mo>,</mo><msub><mi>ğ’™</mi><mi>m</mi></msub><mo stretchy="false">}</mo></mrow></math> are <span class="ltx_text ltx_font_italic">independently and identically distributed (i.i.d.)</span> according to an underlying distribution <math id="S1.I1.i1.p1.m2" class="ltx_Math" alttext="\mathcal{D}" display="inline"><mi class="ltx_font_mathcaligraphic">ğ’Ÿ</mi></math> over the domain set <math id="S1.I1.i1.p1.m3" class="ltx_Math" alttext="\mathcal{X}" display="inline"><mi class="ltx_font_mathcaligraphic">ğ’³</mi></math>.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S1.I1.i2.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">Finiteness of the hypothesis class: The learning algorithm choses the predictor <math id="S1.I1.i2.p1.m1" class="ltx_Math" alttext="h" display="inline"><mi>h</mi></math> from a finite hypothesis set <math id="S1.I1.i2.p1.m2" class="ltx_Math" alttext="\mathcal{H}" display="inline"><mi class="ltx_font_mathcaligraphic">â„‹</mi></math>, i.e. <math id="S1.I1.i2.p1.m3" class="ltx_Math" alttext="|\mathcal{H}|&lt;\infty" display="inline"><mrow><mrow><mo stretchy="false">|</mo><mi class="ltx_font_mathcaligraphic">â„‹</mi><mo stretchy="false">|</mo></mrow><mo>&lt;</mo><mi mathvariant="normal">âˆ</mi></mrow></math>.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S1.I1.i3.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">The realizable assumption: There exists <math id="S1.I1.i3.p1.m1" class="ltx_Math" alttext="h^{*}\in\mathcal{H}" display="inline"><mrow><msup><mi>h</mi><mo>*</mo></msup><mo>âˆˆ</mo><mi class="ltx_font_mathcaligraphic">â„‹</mi></mrow></math> such that the true loss <math id="S1.I1.i3.p1.m2" class="ltx_Math" alttext="L_{(\mathcal{D},f)}(h^{*})" display="inline"><mrow><msub><mi>L</mi><mrow><mo stretchy="false">(</mo><mi class="ltx_font_mathcaligraphic">ğ’Ÿ</mi><mo>,</mo><mi>f</mi><mo stretchy="false">)</mo></mrow></msub><mo>â¢</mo><mrow><mo stretchy="false">(</mo><msup><mi>h</mi><mo>*</mo></msup><mo stretchy="false">)</mo></mrow></mrow></math> is zero, where <math id="S1.I1.i3.p1.m3" class="ltx_Math" alttext="f" display="inline"><mi>f</mi></math> is the true labeling function (i.e. <math id="S1.I1.i3.p1.m4" class="ltx_Math" alttext="y_{i}=f({\bm{x}}_{i})" display="inline"><mrow><msub><mi>y</mi><mi>i</mi></msub><mo>=</mo><mrow><mi>f</mi><mo>â¢</mo><mrow><mo stretchy="false">(</mo><msub><mi>ğ’™</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow></mrow></mrow></math>). This implies that if we randomly sample a data set <math id="S1.I1.i3.p1.m5" class="ltx_Math" alttext="S\sim\mathcal{D}" display="inline"><mrow><mi>S</mi><mo>âˆ¼</mo><mi class="ltx_font_mathcaligraphic">ğ’Ÿ</mi></mrow></math>, the empirical loss <math id="S1.I1.i3.p1.m6" class="ltx_Math" alttext="L_{S}(h^{*})" display="inline"><mrow><msub><mi>L</mi><mi>S</mi></msub><mo>â¢</mo><mrow><mo stretchy="false">(</mo><msup><mi>h</mi><mo>*</mo></msup><mo stretchy="false">)</mo></mrow></mrow></math> is always 0.</p>
</div>
</li>
</ul>
</div>
<div id="S1.p3" class="ltx_para ltx_noindent">
<p class="ltx_p">Say our training set is <math id="S1.p3.m1" class="ltx_Math" alttext="S=\{({\bm{x}}_{1},y_{1}),\dots,({\bm{x}}_{m},y_{m})\}" display="inline"><mrow><mi>S</mi><mo>=</mo><mrow><mo stretchy="false">{</mo><mrow><mo stretchy="false">(</mo><msub><mi>ğ’™</mi><mn>1</mn></msub><mo>,</mo><msub><mi>y</mi><mn>1</mn></msub><mo stretchy="false">)</mo></mrow><mo>,</mo><mi mathvariant="normal">â€¦</mi><mo>,</mo><mrow><mo stretchy="false">(</mo><msub><mi>ğ’™</mi><mi>m</mi></msub><mo>,</mo><msub><mi>y</mi><mi>m</mi></msub><mo stretchy="false">)</mo></mrow><mo stretchy="false">}</mo></mrow></mrow></math> (which is drawn from <math id="S1.p3.m2" class="ltx_Math" alttext="\mathcal{D}^{m}" display="inline"><msup><mi class="ltx_font_mathcaligraphic">ğ’Ÿ</mi><mi>m</mi></msup></math>), and our learning algorithm is <math id="S1.p3.m3" class="ltx_Math" alttext="A" display="inline"><mi>A</mi></math>. The output of the algorithm is the predictor <math id="S1.p3.m4" class="ltx_Math" alttext="h=A(S)" display="inline"><mrow><mi>h</mi><mo>=</mo><mrow><mi>A</mi><mo>â¢</mo><mrow><mo stretchy="false">(</mo><mi>S</mi><mo stretchy="false">)</mo></mrow></mrow></mrow></math>, we measure the performance of <math id="S1.p3.m5" class="ltx_Math" alttext="h" display="inline"><mi>h</mi></math> over the training set <math id="S1.p3.m6" class="ltx_Math" alttext="S" display="inline"><mi>S</mi></math> using the empirical loss:</p>
<table id="S1.Ex1" class="ltx_equation ltx_eqn_table">

<tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S1.Ex1.m1" class="ltx_Math" alttext="L_{S}(h)=\frac{|\{i\in[m]:h({\bm{x}}_{i})\neq y_{i}\}|}{m}=\frac{1}{m}\sum_{i=%
1}^{m}{\bm{1}}_{[h({\bm{x}}_{i})\neq y_{i}]}." display="block"><mrow><mrow><mrow><msub><mi>L</mi><mi>S</mi></msub><mo>â¢</mo><mrow><mo stretchy="false">(</mo><mi>h</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mfrac><mrow><mo stretchy="false">|</mo><mrow><mo stretchy="false">{</mo><mrow><mi>i</mi><mo>âˆˆ</mo><mrow><mo stretchy="false">[</mo><mi>m</mi><mo stretchy="false">]</mo></mrow></mrow><mo>:</mo><mrow><mrow><mi>h</mi><mo>â¢</mo><mrow><mo stretchy="false">(</mo><msub><mi>ğ’™</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo>â‰ </mo><msub><mi>y</mi><mi>i</mi></msub></mrow><mo stretchy="false">}</mo></mrow><mo stretchy="false">|</mo></mrow><mi>m</mi></mfrac><mo>=</mo><mrow><mfrac><mn>1</mn><mi>m</mi></mfrac><mo>â¢</mo><mrow><munderover><mo largeop="true" movablelimits="false" symmetric="true">âˆ‘</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><msub><mn>ğŸ</mn><mrow><mo stretchy="false">[</mo><mrow><mrow><mi>h</mi><mo>â¢</mo><mrow><mo stretchy="false">(</mo><msub><mi>ğ’™</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo>â‰ </mo><msub><mi>y</mi><mi>i</mi></msub></mrow><mo stretchy="false">]</mo></mrow></msub></mrow></mrow></mrow><mo>.</mo></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</table>
</div>
<div id="S1.p4" class="ltx_para ltx_noindent">
<p class="ltx_p">Once we obtained the predictor <math id="S1.p4.m1" class="ltx_Math" alttext="h" display="inline"><mi>h</mi></math>, we care about its general performance in the environment (over the underlying distribution <math id="S1.p4.m2" class="ltx_Math" alttext="\mathcal{D}" display="inline"><mi class="ltx_font_mathcaligraphic">ğ’Ÿ</mi></math>). The generalization loss (or the true error, the risk) is measured by the probability that it predicts wrongly for a random sample:</p>
<table id="S1.Ex2" class="ltx_equation ltx_eqn_table">

<tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S1.Ex2.m1" class="ltx_Math" alttext="L_{(\mathcal{D},f)}(h)=\Pr_{{\bm{x}}\sim\mathcal{D}}[h({\bm{x}})\neq f({\bm{x}%
})]=\mathcal{D}(\{{\bm{x}}:h({\bm{x}})\neq f({\bm{x}})\})," display="block"><mrow><mrow><mrow><msub><mi>L</mi><mrow><mo stretchy="false">(</mo><mi class="ltx_font_mathcaligraphic">ğ’Ÿ</mi><mo>,</mo><mi>f</mi><mo stretchy="false">)</mo></mrow></msub><mo>â¢</mo><mrow><mo stretchy="false">(</mo><mi>h</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><munder><mi>Pr</mi><mrow><mi>ğ’™</mi><mo>âˆ¼</mo><mi class="ltx_font_mathcaligraphic">ğ’Ÿ</mi></mrow></munder><mo>â¡</mo><mrow><mo stretchy="false">[</mo><mrow><mrow><mi>h</mi><mo>â¢</mo><mrow><mo stretchy="false">(</mo><mi>ğ’™</mi><mo stretchy="false">)</mo></mrow></mrow><mo>â‰ </mo><mrow><mi>f</mi><mo>â¢</mo><mrow><mo stretchy="false">(</mo><mi>ğ’™</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><mo stretchy="false">]</mo></mrow></mrow><mo>=</mo><mrow><mi class="ltx_font_mathcaligraphic">ğ’Ÿ</mi><mo>â¢</mo><mrow><mo stretchy="false">(</mo><mrow><mo stretchy="false">{</mo><mi>ğ’™</mi><mo>:</mo><mrow><mrow><mi>h</mi><mo>â¢</mo><mrow><mo stretchy="false">(</mo><mi>ğ’™</mi><mo stretchy="false">)</mo></mrow></mrow><mo>â‰ </mo><mrow><mi>f</mi><mo>â¢</mo><mrow><mo stretchy="false">(</mo><mi>ğ’™</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><mo stretchy="false">}</mo></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>,</mo></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</table>
<p class="ltx_p">where <math id="S1.p4.m3" class="ltx_Math" alttext="\mathcal{D}(S)" display="inline"><mrow><mi class="ltx_font_mathcaligraphic">ğ’Ÿ</mi><mo>â¢</mo><mrow><mo stretchy="false">(</mo><mi>S</mi><mo stretchy="false">)</mo></mrow></mrow></math> is defined as the probability that we sample a point <math id="S1.p4.m4" class="ltx_Math" alttext="{\bm{x}}" display="inline"><mi>ğ’™</mi></math> in <math id="S1.p4.m5" class="ltx_Math" alttext="\mathcal{X}" display="inline"><mi class="ltx_font_mathcaligraphic">ğ’³</mi></math> w.r.t. <math id="S1.p4.m6" class="ltx_Math" alttext="\mathcal{D}" display="inline"><mi class="ltx_font_mathcaligraphic">ğ’Ÿ</mi></math> and the point <math id="S1.p4.m7" class="ltx_Math" alttext="{\bm{x}}" display="inline"><mi>ğ’™</mi></math> lies in <math id="S1.p4.m8" class="ltx_Math" alttext="S" display="inline"><mi>S</mi></math>.</p>
</div>
<section id="S1.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection" style="font-size:120%;"> Empirical Risk Minimization (ERM)</h3>

<div id="S1.SS1.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">The objective of a learning algorithm (or a learner) is to minimize the true error w.r.t. <math id="S1.SS1.p1.m1" class="ltx_Math" alttext="\mathcal{D}" display="inline"><mi class="ltx_font_mathcaligraphic">ğ’Ÿ</mi></math> and <math id="S1.SS1.p1.m2" class="ltx_Math" alttext="f" display="inline"><mi>f</mi></math>. However, <math id="S1.SS1.p1.m3" class="ltx_Math" alttext="\mathcal{D}" display="inline"><mi class="ltx_font_mathcaligraphic">ğ’Ÿ</mi></math> and <math id="S1.SS1.p1.m4" class="ltx_Math" alttext="f" display="inline"><mi>f</mi></math> are unkown. The only thing that we can observe is the training set <math id="S1.SS1.p1.m5" class="ltx_Math" alttext="S\sim\mathcal{D}^{m}" display="inline"><mrow><mi>S</mi><mo>âˆ¼</mo><msup><mi class="ltx_font_mathcaligraphic">ğ’Ÿ</mi><mi>m</mi></msup></mrow></math>. Thus a natural algorithm is to minimize the empirical loss:</p>
<table id="S1.Ex3" class="ltx_equation ltx_eqn_table">

<tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S1.Ex3.m1" class="ltx_Math" alttext="h=\operatorname*{argmin}_{h\in\mathcal{H}}L_{S}(h)." display="block"><mrow><mrow><mi>h</mi><mo>=</mo><mrow><mrow><munder><mo movablelimits="false">argmin</mo><mrow><mi>h</mi><mo>âˆˆ</mo><mi class="ltx_font_mathcaligraphic">â„‹</mi></mrow></munder><mo>â¡</mo><msub><mi>L</mi><mi>S</mi></msub></mrow><mo>â¢</mo><mrow><mo stretchy="false">(</mo><mi>h</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>.</mo></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</table>
</div>
<div id="S1.SS1.p2" class="ltx_para ltx_noindent">
<p class="ltx_p">The algorithm is known as the Empirical Risk Minimization (ERM) algorithm, which we denote as <math id="S1.SS1.p2.m1" class="ltx_Math" alttext="{\sf ERM}_{\mathcal{H}}" display="inline"><msub><mi>ğ–¤ğ–±ğ–¬</mi><mi class="ltx_font_mathcaligraphic">â„‹</mi></msub></math>. Though the concept is simple, ERM has a deadly weakness: it may overfit the training set. When the training set <math id="S1.SS1.p2.m2" class="ltx_Math" alttext="S" display="inline"><mi>S</mi></math> cannot reveal the underlying structure of <math id="S1.SS1.p2.m3" class="ltx_Math" alttext="\mathcal{D}" display="inline"><mi class="ltx_font_mathcaligraphic">ğ’Ÿ</mi></math>, the true loss of the ERM predictor will be large. Imagine that if the labels of the samples in <math id="S1.SS1.p2.m4" class="ltx_Math" alttext="S" display="inline"><mi>S</mi></math> are all one, the ERM predictor may predict everything as positive (though it is a very unlikely situation).</p>
</div>
<div id="S1.SS1.p3" class="ltx_para ltx_noindent">
<p class="ltx_p">A common solution to this problem is to restrict the search space of ERM (shrink the size/space of the candidate set <math id="S1.SS1.p3.m1" class="ltx_Math" alttext="\mathcal{H}" display="inline"><mi class="ltx_font_mathcaligraphic">â„‹</mi></math>). To get some intuition, in the most extreme case, say if the hypothesis class <math id="S1.SS1.p3.m2" class="ltx_Math" alttext="\mathcal{H}" display="inline"><mi class="ltx_font_mathcaligraphic">â„‹</mi></math> is the set of all function from <math id="S1.SS1.p3.m3" class="ltx_Math" alttext="\mathcal{X}" display="inline"><mi class="ltx_font_mathcaligraphic">ğ’³</mi></math> to <math id="S1.SS1.p3.m4" class="ltx_Math" alttext="\{0,1\}" display="inline"><mrow><mo stretchy="false">{</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="false">}</mo></mrow></math>, ERM can learn a predictor <math id="S1.SS1.p3.m5" class="ltx_Math" alttext="h" display="inline"><mi>h</mi></math> with zero emprical loss where <math id="S1.SS1.p3.m6" class="ltx_Math" alttext="h" display="inline"><mi>h</mi></math> fits perfectly to the training set (<math id="S1.SS1.p3.m7" class="ltx_Math" alttext="h({\bm{x}}_{i})=y_{i},\forall i\in[m]" display="inline"><mrow><mrow><mrow><mi>h</mi><mo>â¢</mo><mrow><mo stretchy="false">(</mo><msub><mi>ğ’™</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><msub><mi>y</mi><mi>i</mi></msub></mrow><mo>,</mo><mrow><mrow><mo>âˆ€</mo><mi>i</mi></mrow><mo>âˆˆ</mo><mrow><mo stretchy="false">[</mo><mi>m</mi><mo stretchy="false">]</mo></mrow></mrow></mrow></math>). However, the true loss can be arbitrarily bad, if <math id="S1.SS1.p3.m8" class="ltx_Math" alttext="|S|/|\mathcal{X}|" display="inline"><mrow><mrow><mo stretchy="false">|</mo><mi>S</mi><mo stretchy="false">|</mo></mrow><mo>/</mo><mrow><mo stretchy="false">|</mo><mi class="ltx_font_mathcaligraphic">ğ’³</mi><mo stretchy="false">|</mo></mrow></mrow></math> is close to zero. Philosophically, if someone can explain every phenomenon, his explanations are worthless.</p>
</div>
<div id="S1.SS1.p4" class="ltx_para ltx_noindent">
<p class="ltx_p">Though restricting the space of <math id="S1.SS1.p4.m1" class="ltx_Math" alttext="\mathcal{H}" display="inline"><mi class="ltx_font_mathcaligraphic">â„‹</mi></math> seems helpful in hadling the overfitting problem, it also introduced biases. Therefore, ideally we want the choice of <math id="S1.SS1.p4.m2" class="ltx_Math" alttext="\mathcal{H}" display="inline"><mi class="ltx_font_mathcaligraphic">â„‹</mi></math> to be based on some prior knowledge of the problem. As another extreme case, if <math id="S1.SS1.p4.m3" class="ltx_Math" alttext="\mathcal{H}" display="inline"><mi class="ltx_font_mathcaligraphic">â„‹</mi></math> contains only one predictor (say a threshold function), it will surely not overfit <math id="S1.SS1.p4.m4" class="ltx_Math" alttext="S" display="inline"><mi>S</mi></math>, but the true loss is also unlikely to be small. For now, we simply assume that the realizable assumption holds, which means there exists a <math id="S1.SS1.p4.m5" class="ltx_Math" alttext="h^{*}\in\mathcal{H}" display="inline"><mrow><msup><mi>h</mi><mo>*</mo></msup><mo>âˆˆ</mo><mi class="ltx_font_mathcaligraphic">â„‹</mi></mrow></math> such that <math id="S1.SS1.p4.m6" class="ltx_Math" alttext="L_{(\mathcal{D},f)}(h^{*})=0" display="inline"><mrow><mrow><msub><mi>L</mi><mrow><mo stretchy="false">(</mo><mi class="ltx_font_mathcaligraphic">ğ’Ÿ</mi><mo>,</mo><mi>f</mi><mo stretchy="false">)</mo></mrow></msub><mo>â¢</mo><mrow><mo stretchy="false">(</mo><msup><mi>h</mi><mo>*</mo></msup><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mn>0</mn></mrow></math>. Such an <math id="S1.SS1.p4.m7" class="ltx_Math" alttext="\mathcal{H}" display="inline"><mi class="ltx_font_mathcaligraphic">â„‹</mi></math> has perfect prior knowledge on <math id="S1.SS1.p4.m8" class="ltx_Math" alttext="\mathcal{D}" display="inline"><mi class="ltx_font_mathcaligraphic">ğ’Ÿ</mi></math> and <math id="S1.SS1.p4.m9" class="ltx_Math" alttext="f" display="inline"><mi>f</mi></math>, thus is unrealistic. We will see how to remove this assumption later, but for simplicity letâ€™s keep it for now.</p>
</div>
</section>
<section id="S1.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection" style="font-size:120%;"> Finite Hypothesis Class</h3>

<div id="S1.SS2.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">In this section we show that if the i.i.d. assumption and the realizable assumption hold, with sufficient large number of samples, the true loss of the predictor <math id="S1.SS2.p1.m1" class="ltx_Math" alttext="{\sf ERM}_{\mathcal{H}}(S)" display="inline"><mrow><msub><mi>ğ–¤ğ–±ğ–¬</mi><mi class="ltx_font_mathcaligraphic">â„‹</mi></msub><mo>â¢</mo><mrow><mo stretchy="false">(</mo><mi>S</mi><mo stretchy="false">)</mo></mrow></mrow></math> learned from a finite <math id="S1.SS2.p1.m2" class="ltx_Math" alttext="\mathcal{H}" display="inline"><mi class="ltx_font_mathcaligraphic">â„‹</mi></math> is small:</p>
<table id="S1.Ex4" class="ltx_equation ltx_eqn_table">

<tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S1.Ex4.m1" class="ltx_Math" alttext="L_{(\mathcal{D},f)}({\sf ERM}_{\mathcal{H}}(S))\leq\varepsilon\quad\text{with %
probability at least }(1-\delta)." display="block"><mrow><mrow><mrow><msub><mi>L</mi><mrow><mo stretchy="false">(</mo><mi class="ltx_font_mathcaligraphic">ğ’Ÿ</mi><mo>,</mo><mi>f</mi><mo stretchy="false">)</mo></mrow></msub><mo>â¢</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>ğ–¤ğ–±ğ–¬</mi><mi class="ltx_font_mathcaligraphic">â„‹</mi></msub><mo>â¢</mo><mrow><mo stretchy="false">(</mo><mi>S</mi><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>â‰¤</mo><mrow><mi>Îµ</mi><mo mathvariant="italic" separator="true">â€ƒ</mo><mrow><mtext>with probability at leastÂ </mtext><mo>â¢</mo><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>-</mo><mi>Î´</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</table>
</div>
<div id="S1.SS2.p2" class="ltx_para ltx_noindent">
<p class="ltx_p">Intuitively this is true because the training set can reveal more structure of the underlying distribution with more samples. Suppose the training set <math id="S1.SS2.p2.m1" class="ltx_Math" alttext="S" display="inline"><mi>S</mi></math> contains <math id="S1.SS2.p2.m2" class="ltx_Math" alttext="m" display="inline"><mi>m</mi></math> samples drawn i.i.d. from <math id="S1.SS2.p2.m3" class="ltx_Math" alttext="\mathcal{D}" display="inline"><mi class="ltx_font_mathcaligraphic">ğ’Ÿ</mi></math> and let <math id="S1.SS2.p2.m4" class="ltx_Math" alttext="h_{S}={\sf ERM}_{\mathcal{H}}(S)" display="inline"><mrow><msub><mi>h</mi><mi>S</mi></msub><mo>=</mo><mrow><msub><mi>ğ–¤ğ–±ğ–¬</mi><mi class="ltx_font_mathcaligraphic">â„‹</mi></msub><mo>â¢</mo><mrow><mo stretchy="false">(</mo><mi>S</mi><mo stretchy="false">)</mo></mrow></mrow></mrow></math>, we want to upperbound the probabilty</p>
<table id="S1.Ex5" class="ltx_equation ltx_eqn_table">

<tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S1.Ex5.m1" class="ltx_Math" alttext="\Pr_{S\sim\mathcal{D}^{m}}[L_{(\mathcal{D},f)}(h_{S})&gt;\varepsilon]." display="block"><mrow><mrow><munder><mi>Pr</mi><mrow><mi>S</mi><mo>âˆ¼</mo><msup><mi class="ltx_font_mathcaligraphic">ğ’Ÿ</mi><mi>m</mi></msup></mrow></munder><mo>â¡</mo><mrow><mo stretchy="false">[</mo><mrow><mrow><msub><mi>L</mi><mrow><mo stretchy="false">(</mo><mi class="ltx_font_mathcaligraphic">ğ’Ÿ</mi><mo>,</mo><mi>f</mi><mo stretchy="false">)</mo></mrow></msub><mo>â¢</mo><mrow><mo stretchy="false">(</mo><msub><mi>h</mi><mi>S</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo>&gt;</mo><mi>Îµ</mi></mrow><mo stretchy="false">]</mo></mrow></mrow><mo>.</mo></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</table>
</div>
<div id="S1.SS2.p3" class="ltx_para ltx_noindent">
<p class="ltx_p">Let <math id="S1.SS2.p3.m1" class="ltx_Math" alttext="\mathcal{H}_{B}\subset\mathcal{H}" display="inline"><mrow><msub><mi class="ltx_font_mathcaligraphic">â„‹</mi><mi>B</mi></msub><mo>âŠ‚</mo><mi class="ltx_font_mathcaligraphic">â„‹</mi></mrow></math> be the set of all â€œbadâ€ predictors whoes true loss is greater than <math id="S1.SS2.p3.m2" class="ltx_Math" alttext="\varepsilon" display="inline"><mi>Îµ</mi></math>, i.e. for each <math id="S1.SS2.p3.m3" class="ltx_Math" alttext="h_{B}\in\mathcal{H}_{B}" display="inline"><mrow><msub><mi>h</mi><mi>B</mi></msub><mo>âˆˆ</mo><msub><mi class="ltx_font_mathcaligraphic">â„‹</mi><mi>B</mi></msub></mrow></math> we have <math id="S1.SS2.p3.m4" class="ltx_Math" alttext="L_{(\mathcal{D},f)}(h_{B})&gt;\varepsilon" display="inline"><mrow><mrow><msub><mi>L</mi><mrow><mo stretchy="false">(</mo><mi class="ltx_font_mathcaligraphic">ğ’Ÿ</mi><mo>,</mo><mi>f</mi><mo stretchy="false">)</mo></mrow></msub><mo>â¢</mo><mrow><mo stretchy="false">(</mo><msub><mi>h</mi><mi>B</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo>&gt;</mo><mi>Îµ</mi></mrow></math>. For each <math id="S1.SS2.p3.m5" class="ltx_Math" alttext="h_{B}" display="inline"><msub><mi>h</mi><mi>B</mi></msub></math>, we see that
</p>
<table id="S1.Ex6" class="ltx_equation ltx_eqn_table">

<tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S1.Ex6.m1" class="ltx_Math" alttext="\Pr_{{\bm{x}}\sim\mathcal{D}}[h_{B}({\bm{x}})=f({\bm{x}})]=1-L_{(\mathcal{D},f%
)}(h_{B})\leq 1-\varepsilon." display="block"><mrow><mrow><mrow><munder><mi>Pr</mi><mrow><mi>ğ’™</mi><mo>âˆ¼</mo><mi class="ltx_font_mathcaligraphic">ğ’Ÿ</mi></mrow></munder><mo>â¡</mo><mrow><mo stretchy="false">[</mo><mrow><mrow><msub><mi>h</mi><mi>B</mi></msub><mo>â¢</mo><mrow><mo stretchy="false">(</mo><mi>ğ’™</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mi>f</mi><mo>â¢</mo><mrow><mo stretchy="false">(</mo><mi>ğ’™</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><mo stretchy="false">]</mo></mrow></mrow><mo>=</mo><mrow><mn>1</mn><mo>-</mo><mrow><msub><mi>L</mi><mrow><mo stretchy="false">(</mo><mi class="ltx_font_mathcaligraphic">ğ’Ÿ</mi><mo>,</mo><mi>f</mi><mo stretchy="false">)</mo></mrow></msub><mo>â¢</mo><mrow><mo stretchy="false">(</mo><msub><mi>h</mi><mi>B</mi></msub><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>â‰¤</mo><mrow><mn>1</mn><mo>-</mo><mi>Îµ</mi></mrow></mrow><mo>.</mo></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</table>
</div>
<div id="S1.SS2.p4" class="ltx_para ltx_noindent">
<p class="ltx_p">The probability that we learn the specific bad predictor <math id="S1.SS2.p4.m1" class="ltx_Math" alttext="h_{B}" display="inline"><msub><mi>h</mi><mi>B</mi></msub></math> is the probability that its empirical loss on <math id="S1.SS2.p4.m2" class="ltx_Math" alttext="S" display="inline"><mi>S</mi></math> is zero:</p>
<table id="S1.Ex7" class="ltx_equation ltx_eqn_table">

<tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S1.Ex7.m1" class="ltx_Math" alttext="\Pr_{S\sim\mathcal{D}^{m}}[L_{S}(h_{B})=0]\leq(1-\varepsilon)^{m}\leq e^{-%
\varepsilon m}." display="block"><mrow><mrow><mrow><munder><mi>Pr</mi><mrow><mi>S</mi><mo>âˆ¼</mo><msup><mi class="ltx_font_mathcaligraphic">ğ’Ÿ</mi><mi>m</mi></msup></mrow></munder><mo>â¡</mo><mrow><mo stretchy="false">[</mo><mrow><mrow><msub><mi>L</mi><mi>S</mi></msub><mo>â¢</mo><mrow><mo stretchy="false">(</mo><msub><mi>h</mi><mi>B</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mn>0</mn></mrow><mo stretchy="false">]</mo></mrow></mrow><mo>â‰¤</mo><msup><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>-</mo><mi>Îµ</mi></mrow><mo stretchy="false">)</mo></mrow><mi>m</mi></msup><mo>â‰¤</mo><msup><mi>e</mi><mrow><mo>-</mo><mrow><mi>Îµ</mi><mo>â¢</mo><mi>m</mi></mrow></mrow></msup></mrow><mo>.</mo></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</table>
</div>
<div id="S1.SS2.p5" class="ltx_para ltx_noindent">
<p class="ltx_p">By the union bound, the probability that we learn any bad predictor <math id="S1.SS2.p5.m1" class="ltx_Math" alttext="h_{B}\in\mathcal{H}_{B}" display="inline"><mrow><msub><mi>h</mi><mi>B</mi></msub><mo>âˆˆ</mo><msub><mi class="ltx_font_mathcaligraphic">â„‹</mi><mi>B</mi></msub></mrow></math> is:</p>
<table id="S1.Ex8" class="ltx_equation ltx_eqn_table">

<tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S1.Ex8.m1" class="ltx_Math" alttext="\Pr_{S\sim\mathcal{D}^{m}}[\exists h_{B}\in\mathcal{H}_{B}:L_{S}(h_{B})=0]\leq%
\sum_{h_{B}\in\mathcal{H}_{B}}\Pr_{S\sim\mathcal{D}^{m}}[L_{S}(h_{B})=0]\leq|%
\mathcal{H}_{B}|e^{-\varepsilon m}\leq|\mathcal{H}|e^{-\varepsilon m}," display="block"><mrow><mrow><mrow><munder><mi>Pr</mi><mrow><mi>S</mi><mo>âˆ¼</mo><msup><mi class="ltx_font_mathcaligraphic">ğ’Ÿ</mi><mi>m</mi></msup></mrow></munder><mo>â¡</mo><mrow><mo stretchy="false">[</mo><mrow><mrow><mrow><mo>âˆƒ</mo><msub><mi>h</mi><mi>B</mi></msub></mrow><mo>âˆˆ</mo><msub><mi class="ltx_font_mathcaligraphic">â„‹</mi><mi>B</mi></msub></mrow><mo>:</mo><mrow><mrow><msub><mi>L</mi><mi>S</mi></msub><mo>â¢</mo><mrow><mo stretchy="false">(</mo><msub><mi>h</mi><mi>B</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mn>0</mn></mrow></mrow><mo stretchy="false">]</mo></mrow></mrow><mo>â‰¤</mo><mrow><munder><mo largeop="true" movablelimits="false" symmetric="true">âˆ‘</mo><mrow><msub><mi>h</mi><mi>B</mi></msub><mo>âˆˆ</mo><msub><mi class="ltx_font_mathcaligraphic">â„‹</mi><mi>B</mi></msub></mrow></munder><mrow><munder><mi>Pr</mi><mrow><mi>S</mi><mo>âˆ¼</mo><msup><mi class="ltx_font_mathcaligraphic">ğ’Ÿ</mi><mi>m</mi></msup></mrow></munder><mo>â¡</mo><mrow><mo stretchy="false">[</mo><mrow><mrow><msub><mi>L</mi><mi>S</mi></msub><mo>â¢</mo><mrow><mo stretchy="false">(</mo><msub><mi>h</mi><mi>B</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mn>0</mn></mrow><mo stretchy="false">]</mo></mrow></mrow></mrow><mo>â‰¤</mo><mrow><mrow><mo stretchy="false">|</mo><msub><mi class="ltx_font_mathcaligraphic">â„‹</mi><mi>B</mi></msub><mo stretchy="false">|</mo></mrow><mo>â¢</mo><msup><mi>e</mi><mrow><mo>-</mo><mrow><mi>Îµ</mi><mo>â¢</mo><mi>m</mi></mrow></mrow></msup></mrow><mo>â‰¤</mo><mrow><mrow><mo stretchy="false">|</mo><mi class="ltx_font_mathcaligraphic">â„‹</mi><mo stretchy="false">|</mo></mrow><mo>â¢</mo><msup><mi>e</mi><mrow><mo>-</mo><mrow><mi>Îµ</mi><mo>â¢</mo><mi>m</mi></mrow></mrow></msup></mrow></mrow><mo>,</mo></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</table>
<p class="ltx_p">which is equivalent to:</p>
<table id="S1.Ex9" class="ltx_equation ltx_eqn_table">

<tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S1.Ex9.m1" class="ltx_Math" alttext="\Pr_{S\sim\mathcal{D}^{m}}[L_{(\mathcal{D},f)}(h_{S})&gt;\varepsilon]\leq|%
\mathcal{H}|e^{-\varepsilon m}." display="block"><mrow><mrow><mrow><munder><mi>Pr</mi><mrow><mi>S</mi><mo>âˆ¼</mo><msup><mi class="ltx_font_mathcaligraphic">ğ’Ÿ</mi><mi>m</mi></msup></mrow></munder><mo>â¡</mo><mrow><mo stretchy="false">[</mo><mrow><mrow><msub><mi>L</mi><mrow><mo stretchy="false">(</mo><mi class="ltx_font_mathcaligraphic">ğ’Ÿ</mi><mo>,</mo><mi>f</mi><mo stretchy="false">)</mo></mrow></msub><mo>â¢</mo><mrow><mo stretchy="false">(</mo><msub><mi>h</mi><mi>S</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo>&gt;</mo><mi>Îµ</mi></mrow><mo stretchy="false">]</mo></mrow></mrow><mo>â‰¤</mo><mrow><mrow><mo stretchy="false">|</mo><mi class="ltx_font_mathcaligraphic">â„‹</mi><mo stretchy="false">|</mo></mrow><mo>â¢</mo><msup><mi>e</mi><mrow><mo>-</mo><mrow><mi>Îµ</mi><mo>â¢</mo><mi>m</mi></mrow></mrow></msup></mrow></mrow><mo>.</mo></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</table>
</div>
<div id="S1.SS2.p6" class="ltx_para ltx_noindent">
<p class="ltx_p">We conclude that:</p>
</div>
<div id="Thmclaim1" class="ltx_theorem ltx_theorem_corollary">
<h6 class="ltx_title ltx_runin ltx_title_theorem" style="color:#0000FF;">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold">Corollary 1</span></span><span class="ltx_text ltx_font_bold">.</span>
</h6>
<div id="Thmclaim1.p1" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_italic" style="color:#0000FF;">Let <math id="Thmclaim1.p1.m1" class="ltx_Math" alttext="\mathcal{H}" display="inline"><mi class="ltx_font_mathcaligraphic" mathcolor="#0000FF">â„‹</mi></math> be a finite hypothesis class. Let <math id="Thmclaim1.p1.m2" class="ltx_Math" alttext="\delta\in(0,1)" display="inline"><mrow><mi mathcolor="#0000FF">Î´</mi><mo mathcolor="#0000FF" mathvariant="normal">âˆˆ</mo><mrow><mo mathcolor="#0000FF" mathvariant="normal" stretchy="false">(</mo><mn mathcolor="#0000FF" mathvariant="normal">0</mn><mo mathcolor="#0000FF" mathvariant="normal">,</mo><mn mathcolor="#0000FF" mathvariant="normal">1</mn><mo mathcolor="#0000FF" mathvariant="normal" stretchy="false">)</mo></mrow></mrow></math> and <math id="Thmclaim1.p1.m3" class="ltx_Math" alttext="\varepsilon&gt;0" display="inline"><mrow><mi mathcolor="#0000FF">Îµ</mi><mo mathcolor="#0000FF" mathvariant="normal">&gt;</mo><mn mathcolor="#0000FF" mathvariant="normal">0</mn></mrow></math>, and let <math id="Thmclaim1.p1.m4" class="ltx_Math" alttext="m" display="inline"><mi mathcolor="#0000FF">m</mi></math> be an integer satisfies</span></p>
<table id="S1.Ex10" class="ltx_equation ltx_eqn_table">

<tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S1.Ex10.m1" class="ltx_Math" alttext="m\geq\frac{\log(|\mathcal{H}|/\delta)}{\varepsilon}." display="block"><mrow><mrow><mi mathcolor="#0000FF">m</mi><mo mathcolor="#0000FF">â‰¥</mo><mfrac mathcolor="#0000FF"><mrow><mi mathcolor="#0000FF">log</mi><mo mathcolor="#0000FF">â¡</mo><mrow><mo mathcolor="#0000FF" stretchy="false">(</mo><mrow><mrow><mo mathcolor="#0000FF" stretchy="false">|</mo><mi class="ltx_font_mathcaligraphic" mathcolor="#0000FF">â„‹</mi><mo mathcolor="#0000FF" stretchy="false">|</mo></mrow><mo mathcolor="#0000FF">/</mo><mi mathcolor="#0000FF">Î´</mi></mrow><mo mathcolor="#0000FF" stretchy="false">)</mo></mrow></mrow><mi mathcolor="#0000FF">Îµ</mi></mfrac></mrow><mo mathcolor="#0000FF">.</mo></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</table>
<p class="ltx_p"><span class="ltx_text ltx_font_italic" style="color:#0000FF;">Then, for any distribution <math id="Thmclaim1.p1.m5" class="ltx_Math" alttext="\mathcal{D}" display="inline"><mi class="ltx_font_mathcaligraphic" mathcolor="#0000FF">ğ’Ÿ</mi></math> and for any labeling function <math id="Thmclaim1.p1.m6" class="ltx_Math" alttext="f" display="inline"><mi mathcolor="#0000FF">f</mi></math>, if the realizable assumption holds, with probability at least <math id="Thmclaim1.p1.m7" class="ltx_Math" alttext="(1-\delta)" display="inline"><mrow><mo mathcolor="#0000FF" mathvariant="normal" stretchy="false">(</mo><mrow><mn mathcolor="#0000FF" mathvariant="normal">1</mn><mo mathcolor="#0000FF" mathvariant="normal">-</mo><mi mathcolor="#0000FF">Î´</mi></mrow><mo mathcolor="#0000FF" mathvariant="normal" stretchy="false">)</mo></mrow></math>, we have</span></p>
<table id="S1.Ex11" class="ltx_equation ltx_eqn_table">

<tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S1.Ex11.m1" class="ltx_Math" alttext="L_{(\mathcal{D},f)}({\sf ERM}_{\mathcal{H}}(S))\leq\varepsilon." display="block"><mrow><mrow><mrow><msub><mi mathcolor="#0000FF">L</mi><mrow><mo mathcolor="#0000FF" stretchy="false">(</mo><mi class="ltx_font_mathcaligraphic" mathcolor="#0000FF">ğ’Ÿ</mi><mo mathcolor="#0000FF">,</mo><mi mathcolor="#0000FF">f</mi><mo mathcolor="#0000FF" stretchy="false">)</mo></mrow></msub><mo mathcolor="#0000FF">â¢</mo><mrow><mo mathcolor="#0000FF" stretchy="false">(</mo><mrow><msub><mi mathcolor="#0000FF">ğ–¤ğ–±ğ–¬</mi><mi class="ltx_font_mathcaligraphic" mathcolor="#0000FF">â„‹</mi></msub><mo mathcolor="#0000FF">â¢</mo><mrow><mo mathcolor="#0000FF" stretchy="false">(</mo><mi mathcolor="#0000FF">S</mi><mo mathcolor="#0000FF" stretchy="false">)</mo></mrow></mrow><mo mathcolor="#0000FF" stretchy="false">)</mo></mrow></mrow><mo mathcolor="#0000FF">â‰¤</mo><mi mathcolor="#0000FF">Îµ</mi></mrow><mo mathcolor="#0000FF">.</mo></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</table>
</div>
</div>
<div id="S1.SS2.p7" class="ltx_para">
<p class="ltx_p"></p>
</div>
</section>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul id="bib.L1" class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem ltx_bib_book">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">S. Shalev-Shwartz and S. Ben-David</span><span class="ltx_text ltx_bib_year"> (2014)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Understanding machine learning - from theory to algorithms.</span>.
</span>
<span class="ltx_bibblock"> <span class="ltx_text ltx_bib_publisher">Cambridge University Press</span>.
</span>
</li>
</ul>
</section>
</article>
</div>
</div>
</body>
</html>
